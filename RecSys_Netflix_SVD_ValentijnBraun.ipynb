{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The applications of recommender systems in businesses have become increasingly popular. Recommender systems apply various sources of information including demographics, social, and preferences to provide users with tailored recommended items. Moreover, the favoured technique for building recommender systems is Collaborative filtering. This technique is further divided into three main categories including Memory-Based, Model-Based, and Hybrid-BAsed. Therefore, as large businesses realise the advantages of recommending personalized items to users, the research for techniques, sources of information, and implementation grows. Netflix is among the big organisations interested in the expansion of recommmender system. The application of recommender systems at Netflix is widely known, however improvements are continiously being investigated to provide users with the best movie and series recommendations. Thus, this notebook aims to investigate the application of collaborative filtering techniques for Netflix.      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which type of RecSys based on CF could Netflix user to provide the most accurate recommendations to users?\n",
    "\n",
    "- What are the different type of RecSys based on CF?\n",
    "- Which types could be used for Netflix' dataset?\n",
    "- How do KNN and SVD compare?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Importing Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Used for visualisations in the EDA\n",
    "import seaborn as sns\n",
    "\n",
    "# Used in reducing the memory storage of sparse matrices\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Used for creating a KNN and SVD RecSys model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Used for performance evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# As working with NaN values in matrices overwelmed the output with warnings \n",
    "# these warnings will be ignored.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Netflix Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Import Source Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to append movieId to each record in all of the source files if this has not been executed earlier. This will allow all the source files to be loaded into a dataframe with one line of code and without having to add the movieId seperately before concatting the sourcefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_netflix_source():\n",
    "    x = 0\n",
    "    string = \",\"+str(x)\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            # Check if the first line of the first file already has been formatted\n",
    "            # If yes the formatting will be skipped\n",
    "            if(f.readlines()[0] == '1:,1\\n'):\n",
    "                print('Source is already formatted, continuing \\n')\n",
    "                return\n",
    "            else:\n",
    "                print('Started formatting source files \\n')\n",
    "                # Append a ',', movieId and \\n (newline) to each line \n",
    "                file_lines = [''.join([x.strip(), string, '\\n']) for x in f.readlines()]\n",
    "        with open(os.path.join(directory, filename), 'w') as f:\n",
    "            # Save the formatted file\n",
    "            f.writelines(file_lines)\n",
    "            print('Completed formatting source files')\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the movie dataframe by concatting all the sourcefiles without their title (skiprows=1). Concluding with naming the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "directory = \"/Users/vbraun/Downloads/training_set/\"\n",
    "\n",
    "# \n",
    "format_netflix_source()\n",
    "\n",
    "print('Started concatting all source files to DataFrame \\n')\n",
    "# Importing all different files into one dataframe (including itemId)\n",
    "# As the files do not have a header and differ between each other 'header' = None and the first row will be skipped\n",
    "netflix_df = pd.concat(pd.read_csv(os.path.join(directory, fname), skiprows=1,header=None) for fname in os.listdir(directory)).rename(columns={0:'userId',1:'rating',2:'date',3:'itemId'})\n",
    "print('Completed concatting all source files to DataFrame \\n')\n",
    "\n",
    "# Dropping the date column as this is not relevant for this recommender system\n",
    "netflix_df = netflix_df.drop(columns='date')\n",
    "\n",
    "display(netflix_df.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Data Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow for faster development a debugging variable is used. If debugging is True the dataset will only consist of the first 100 movies. For the final model, debugging will be set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging = True\n",
    "# \n",
    "if debugging == True:\n",
    "    print('Debugging is set to True: limiting the dataset to the first 100 movies')\n",
    "    filtered_netflix_df = netflix_df[netflix_df['itemId'] <= 100]\n",
    "else:\n",
    "    filtered_netflix_df = netflix_df\n",
    "print('Length of selected dataset: {0} \\n'.format(len(filtered_netflix_df)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to filter the dataset based on activity and reduce the sparsity of the data, the data will be grouped and filtered based on movies and users. The datasets will show how many ratings each movie has gotten and how many rating each user has given."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the sparcity of data in the dataset, we will filter out the users that have rated fewer than 5% of the total amount of movies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the movies that have been rated by fewer than 50 people will be filtered out of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "filtered_movie_count = filtered_netflix_df[['itemId','userId']].groupby('itemId').count().reset_index().rename(columns={'userId':'user_count'})\n",
    "filtered_user_count = filtered_netflix_df[['itemId','userId']].groupby('userId').count().reset_index().rename(columns={'itemId':'item_count'})\n",
    "# display(filtered_movie_count.head(3),filtered_user_count.head(3))\n",
    "\n",
    "required_rated_percentage = 0.05\n",
    "print('Filtering out users that have rated less than {0}% of all items'.format(int(required_rated_percentage*100)))\n",
    "# Look if the userId exists when the total amount of items rated by a user (filtered_user_count['item_count']), divided by the total amount of movies (len(filtered_movie_count)' is bigger than the required_rated_percentage (default = 0.05)\n",
    "filtered_netflix_df = filtered_netflix_df[filtered_netflix_df['userId'].isin(filtered_user_count[filtered_user_count['item_count']/len(filtered_movie_count) > required_rated_percentage]['userId'])]\n",
    "print('Length of filtered dataset: {0} \\n'.format(len(filtered_netflix_df)))\n",
    "\n",
    "print('Filtering out movies that have been rated by fewer than {0} users'.format(50))\n",
    "# \n",
    "filtered_netflix_df = filtered_netflix_df[filtered_netflix_df['itemId'].isin(filtered_movie_count[filtered_movie_count['user_count']>50]['itemId'])]\n",
    "print('Length of filtered dataset:',len(filtered_netflix_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Jester Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Import Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "jester_items = pd.read_csv(r'C:\\Users\\vbraun\\Downloads\\SDM-Datasets\\jester_items.csv')\n",
    "jester_ratings = pd.read_csv(r'C:\\Users\\vbraun\\Downloads\\SDM-Datasets\\jester_ratings.csv')\n",
    "\n",
    "jester_df = jester_ratings.rename(columns={'jokeId':'itemId'})\n",
    "jester_df['rating'] = jester_df['rating'] + 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "jester_item_count = jester_df[['itemId','userId']].groupby('itemId').count().reset_index().rename(columns={'userId':'user_count'})\n",
    "jester_user_count = jester_df[['itemId','userId']].groupby('userId').count().reset_index().rename(columns={'itemId':'item_count'})\n",
    "\n",
    "required_rated_percentage = 0.05\n",
    "filtered_jester_df = jester_df[jester_df['userId'].isin(jester_user_count[jester_user_count['item_count']/len(jester_item_count) > required_rated_percentage]['userId'])]\n",
    "filtered_jester_df = filtered_jester_df[filtered_jester_df['itemId'].isin(jester_item_count[jester_item_count['user_count']>20]['itemId'])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 EDA Netflix Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The filtered dataset has', filtered_df['userId'].nunique(), 'unique users')\n",
    "# print('The filtered dataset has', filtered_df['itemId'].nunique(), 'unique movies')\n",
    "# print('The filtered dataset has', filtered_df['rating'].nunique(), 'unique ratings')\n",
    "# print('The unique ratings are', sorted(filtered_df['rating'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(filtered_df.head(),filtered_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Amount of NaN values in the dataset:',filtered_df.loc[lambda x: x.isnull().any(axis=1)].shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph shows for each movie (as a dot) what its mean rating is in comparison to the total amount of ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt = sns.jointplot(x='rating_mean', y='rating_amount', data=filtered_df.groupby('itemId').agg(rating_mean = ('rating', 'mean'), rating_amount = ('rating', 'count')).reset_index())\n",
    "# plt.fig.suptitle(\"Rating Mean and Rating Amount of Movies\")\n",
    "# plt.fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, movies with low mean ratings have generally been rated a low number of times in relation to movies with a mean rating higher than 3.0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next graph shows the same variables as the graph seen above for each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt = sns.jointplot(x='rating_mean', y='rating_amount', data=filtered_df.groupby('userId').agg(rating_mean = ('rating', 'mean'), rating_amount = ('rating', 'count')).reset_index())\n",
    "# plt.fig.suptitle(\"Rating Mean and Rating Amount of Users\")\n",
    "# plt.fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph for users show that there are several outliers of users that have rated many movies while having a low mean of their ratings. Additionally, the graph shows that mean of the rating mean approaches normality. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 EDA Jester Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The filtered dataset has', filtered_df['userId'].nunique(), 'unique users')\n",
    "# print('The filtered dataset has', filtered_df['itemId'].nunique(), 'unique movies')\n",
    "# print('The filtered dataset has', filtered_df['rating'].nunique(), 'unique ratings')\n",
    "# print('The unique ratings are', sorted(filtered_df['rating'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(filtered_df.head(),filtered_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Amount of NaN values in the dataset:',filtered_df.loc[lambda x: x.isnull().any(axis=1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt = sns.jointplot(x='rating_mean', y='rating_amount', data=filtered_df.groupby('itemId').agg(rating_mean = ('rating', 'mean'), rating_amount = ('rating', 'count')).reset_index())\n",
    "# plt.fig.suptitle(\"Rating Mean and Rating Amount of Movies\")\n",
    "# plt.fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt = sns.jointplot(x='rating_mean', y='rating_amount', data=filtered_df.groupby('userId').agg(rating_mean = ('rating', 'mean'), rating_amount = ('rating', 'count')).reset_index())\n",
    "# plt.fig.suptitle(\"Rating Mean and Rating Amount of Users\")\n",
    "# plt.fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(pred, truth):\n",
    "    pred = pred[truth.nonzero()].flatten()\n",
    "    truth = truth[truth.nonzero()].flatten()\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(pred,truth))\n",
    "    mae = mean_absolute_error(pred,truth)\n",
    "    \n",
    "    return rmse, mae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 K Nearest Neighbors (KNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(filtered_df):\n",
    "\n",
    "    sparse_matrix = csr_matrix(filtered_df.pivot_table(index='userId', columns='itemId', values='rating').fillna(0).values)\n",
    "    sparse_matrix.check_format\n",
    "\n",
    "    train_data, test_data = train_test_split(sparse_matrix, test_size=.10)\n",
    "    test_data, validation_data = train_test_split(test_data, test_size=.50)\n",
    "\n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_knn_prediction(train_data, test_data, k = 5, metric = 'cosine', n_neighbors = 20):\n",
    "    knn_model = NearestNeighbors(metric=metric,algorithm='brute',n_neighbors=n_neighbors,n_jobs=-1)\n",
    "\n",
    "    knn_model_fitted = knn_model.fit(train_data.toarray())\n",
    "    distance, indices = knn_model_fitted.kneighbors(test_data.toarray(),k)\n",
    "\n",
    "    raw_recommends = sorted(list(zip(indices.squeeze().tolist(), distance.squeeze().tolist())), key=lambda x: x[1])[:0:-1]\n",
    "    knn_prediction = []\n",
    "    for i, (idx, dist) in enumerate(raw_recommends):\n",
    "        td = pd.DataFrame(train_data.toarray())\n",
    "        sim_users = np.array(td[td.index.isin(idx)])\n",
    "        sim_users[sim_users == 0] = np.nan\n",
    "        average_rat = np.nan_to_num(np.nanmean(sim_users,axis=0))\n",
    "        knn_prediction.append(average_rat)\n",
    "    \n",
    "    # rmse, mae = evaluate_predictions(test_data.toarray(),np.array(knn_prediction))\n",
    "    \n",
    "    return np.array(knn_prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 KNN Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_knn(train_data, validation_data):\n",
    "    n_neighbors = [5,10,20,50]\n",
    "    recommendation_amount = [3,5,10]\n",
    "    metric = ['euclidean','manhattan','cosine','minkowski']\n",
    "\n",
    "    hpt_results = []\n",
    "    for met in metric:\n",
    "        for k in recommendation_amount:\n",
    "            for n in n_neighbors:\n",
    "                rmse, mae = evaluate_predictions(validation_data.toarray(),calculate_knn_prediction(train_data = train_data, test_data = validation_data, metric = met, k = k, n_neighbors = n))\n",
    "                print(rmse,met,k,n)\n",
    "                hpt_results.append([rmse,met,k,n])\n",
    "\n",
    "    best_parameters_knn = sorted(hpt_results, key=lambda x: x[0])[0]\n",
    "    print(best_parameters)\n",
    "\n",
    "    return best_parameters_knn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create NearestNeighbors model\n",
    "1. Fit the model with train data\n",
    "1. Use kneighbors to find the k amount of neighbors of the jokes in the test data\n",
    "1. Calculate the prediction by taking the average score of the k most similar jokes\n",
    "1. Evaluate the model by comparing the actual ratings with the predicted ratings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm is set at brute (force) because the inputdata is sparse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Singular Value Decomposition (SVD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 SVD Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot the dataset into a matrix with index='userId', columns='itemId', values='rating' in order to later perform user-based collaborative filtering. Moreover, fill_value = 0 in order to remove NaN values and save them as 0. Finally, the matrix is directly stored as a sparse matrix to save memory, instead of first saving the entire matrix into memory. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy.sparse.linalg.svds was used to perform a partial singular value decomposition of a sparse matrix. This function allows us to specify 'k' which is the number of singular values and singular vectors that have to be computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_svd_prediction(data, k = 5):\n",
    "    # Performing the SVD matrix factorisation giving: u (m x r) orthogonal matrix, \n",
    "    # s (r x r) diagonal matrix, and vt(ransposed) (r x n) orthogonal matrix.\n",
    "    u, s, vt = svds(data.toarray(), k = k)\n",
    "\n",
    "    # A diagonal matrix has to be created for s in order to recreate a matrix from u, s, and vt\n",
    "    s_diagonal = np.diag(s)\n",
    "\n",
    "    # Recreate the matrix by performing matrix multiplications of u, s, and vt\n",
    "    predictions = np.dot(np.dot(u, s_diagonal), vt)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of the recommendations following SVD we only need the $\\hat{y}$ of existing $y$. Therefore, all other values will be filtered out of the prediction dataset by using pred[truth.nonzero()]. Afterwards we are able to evaluate the performance of our model by comparing $\\hat{y}$ with their corresponding $y$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 SVD Hyper Parameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different k values lead to different predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform hyperparameter tuning to find the k with the lowest rmse. For each k we will perform multiple iterations, in which a random sample of the data will be masked and used to calculate the rmse. The rmse of a k value will be the average rmse of all iterations of that k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_svd(dataset, iterations = 10):\n",
    "    results = []\n",
    "\n",
    "    print('Calculating the average rmse over {0} iterations'.format(iterations))\n",
    "\n",
    "    k_list = [1,2,3,4,5,6,7,8,9,10,20,30,50,80]\n",
    "\n",
    "    for k in k_list:\n",
    "        rmse_list = []\n",
    "        for i in range(0,iterations):\n",
    "            dataset_ex_masked, masked_data = train_test_split(dataset, test_size=.05)\n",
    "\n",
    "            dataset_ex_masked_csr = csr_matrix(dataset_ex_masked.pivot_table(index='userId', columns='itemId', values='rating').fillna(0).values)\n",
    "            masked_data_csr = csr_matrix(masked_data.pivot_table(index='userId', columns='itemId', values='rating').fillna(0).values)\n",
    "\n",
    "            rmse, mae = evaluate_predictions(calculate_svd_prediction(dataset_ex_masked_csr,k),masked_data_csr.toarray())\n",
    "            \n",
    "            rmse_list.append(rmse)\n",
    "    \n",
    "        results.append([k,(sum(rmse_list)/len(rmse_list))])\n",
    "        print('For k = {0}, the average rmse = {1}'.format(k,(sum(rmse_list)/len(rmse_list))))\n",
    "\n",
    "    best_parameters_svd = sorted(results, key=lambda x: x[1])[0]\n",
    "    print('The rmse is lowest for k = {0} at = {1}'.format(best_parameters_svd[0],best_parameters_svd[1]))\n",
    "    \n",
    "    return best_parameters_svd\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# final_csr_matrix = csr_matrix(filtered_df.pivot_table(index='userId', columns='itemId', values='rating').fillna(0).values)\n",
    "# predictions = hyper_svd(final_csr_matrix,best_parameters_svd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend_for_user = 650\n",
    "\n",
    "# user_pred_df = pd.DataFrame(predictions)\n",
    "# user_sel_pred_df = user_pred_df.loc[recommend_for_user].sort_values(ascending=False)\n",
    "\n",
    "# user_df = pd.DataFrame(final_csr_matrix.toarray())\n",
    "# selected = pd.DataFrame(user_df.loc[recommend_for_user])\n",
    "# rated_movies = selected.loc[~(selected==0).all(axis=1)].index.values.tolist()\n",
    "\n",
    "# recommended_movies = user_sel_pred_df.loc[~user_sel_pred_df.index.isin(rated_movies)]\n",
    "\n",
    "# print('Rated items are:',rated_movies)\n",
    "# print(recommended_movies[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Performance Netflix Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Results KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_train, knn_validation, knn_test = train_test(filtered_netflix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean\n",
      "manhattan\n",
      "cosine\n",
      "minkowski\n",
      "[2.4038460040135923, 'euclidean', 10, 5]\n"
     ]
    }
   ],
   "source": [
    "best_params_knn = hyper_parameter_tuning_knn(train_data = knn_train, validation_data = knn_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.4111006751666118, 2.1145233715352387)"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_predictions(knn_test.toarray(),calculate_knn_prediction(train_data = knn_train, test_data = knn_test, metric = best_params_knn[1], k = best_params_knn[2], n_neighbors = best_params_knn[3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Results SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the average rmse over 10 iterations\n",
      "For k = 1, the average rmse = 3.3015731898684115\n",
      "For k = 2, the average rmse = 3.2872733279246105\n",
      "For k = 3, the average rmse = 3.242653750720451\n",
      "For k = 4, the average rmse = 3.294263396501619\n",
      "For k = 5, the average rmse = 3.300066123582045\n",
      "For k = 6, the average rmse = 3.3099805502183357\n",
      "For k = 7, the average rmse = 3.286975095455766\n",
      "For k = 8, the average rmse = 3.2822585245805724\n",
      "For k = 9, the average rmse = 3.2961219122878553\n",
      "For k = 10, the average rmse = 3.284985285116142\n",
      "For k = 20, the average rmse = 3.329788566274631\n",
      "For k = 30, the average rmse = 3.3324355402642007\n",
      "For k = 50, the average rmse = 3.3649706393756134\n",
      "For k = 80, the average rmse = 3.388405900587698\n",
      "The rmse is lowest for k = 3 at = 3.242653750720451\n"
     ]
    }
   ],
   "source": [
    "best_params_svd = hyper_parameter_tuning_svd(dataset = filtered_netflix_df, iterations = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Comparison KNN & SVD for Netflix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3.1 Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3.2 Recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Performance Jester Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Results KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_train, knn_validation, knn_test = train_test(filtered_jester_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.151903699790237 euclidean 3 5\n",
      "10.151903699790237 euclidean 3 10\n",
      "10.151903699790237 euclidean 3 20\n",
      "10.151903699790237 euclidean 3 50\n",
      "10.014150049430668 euclidean 5 5\n",
      "10.014150049430668 euclidean 5 10\n",
      "10.014150049430668 euclidean 5 20\n",
      "10.014150049430668 euclidean 5 50\n",
      "9.82456844739558 euclidean 10 5\n",
      "9.82456844739558 euclidean 10 10\n",
      "9.82456844739558 euclidean 10 20\n",
      "9.82456844739558 euclidean 10 50\n",
      "10.731054298178492 manhattan 3 5\n",
      "10.731054298178492 manhattan 3 10\n",
      "10.731054298178492 manhattan 3 20\n",
      "10.731054298178492 manhattan 3 50\n",
      "10.626282985091194 manhattan 5 5\n",
      "10.626282985091194 manhattan 5 10\n",
      "10.626282985091194 manhattan 5 20\n",
      "10.626282985091194 manhattan 5 50\n",
      "10.516305141287347 manhattan 10 5\n",
      "10.516305141287347 manhattan 10 10\n",
      "10.516305141287347 manhattan 10 20\n",
      "10.516305141287347 manhattan 10 50\n",
      "11.122161818474266 cosine 3 5\n",
      "11.122161818474266 cosine 3 10\n",
      "11.122161818474266 cosine 3 20\n",
      "11.122161818474266 cosine 3 50\n",
      "11.049417775495579 cosine 5 5\n",
      "11.049417775495579 cosine 5 10\n",
      "11.049417775495579 cosine 5 20\n",
      "11.049417775495579 cosine 5 50\n",
      "10.853990198636177 cosine 10 5\n",
      "10.853990198636177 cosine 10 10\n",
      "10.853990198636177 cosine 10 20\n",
      "10.853990198636177 cosine 10 50\n",
      "10.151903699790237 minkowski 3 5\n",
      "10.151903699790237 minkowski 3 10\n",
      "10.151903699790237 minkowski 3 20\n",
      "10.151903699790237 minkowski 3 50\n",
      "10.014150049430668 minkowski 5 5\n",
      "10.014150049430668 minkowski 5 10\n",
      "10.014150049430668 minkowski 5 20\n",
      "10.014150049430668 minkowski 5 50\n",
      "9.82456844739558 minkowski 10 5\n",
      "9.82456844739558 minkowski 10 10\n",
      "9.82456844739558 minkowski 10 20\n",
      "9.82456844739558 minkowski 10 50\n",
      "[2.4038460040135923, 'euclidean', 10, 5]\n"
     ]
    }
   ],
   "source": [
    "best_params_knn = hyper_parameter_tuning_knn(train_data = knn_train, validation_data = knn_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.882721365179828, 8.638029386025508)"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_predictions(knn_test.toarray(),calculate_knn_prediction(train_data = knn_train, test_data = knn_test, metric = best_params_knn[1], k = best_params_knn[2], n_neighbors = best_params_knn[3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Results SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the average rmse over 2 iterations\n",
      "For k = 1, the average rmse = 10.197680846160697\n",
      "For k = 2, the average rmse = 10.177194623331173\n",
      "For k = 3, the average rmse = 10.28298939596786\n",
      "For k = 4, the average rmse = 10.320629616808938\n",
      "For k = 5, the average rmse = 10.365160655511655\n",
      "For k = 6, the average rmse = 10.381040518946396\n",
      "For k = 7, the average rmse = 10.384607354708603\n",
      "For k = 8, the average rmse = 10.430343480008583\n",
      "For k = 9, the average rmse = 10.469413237252866\n",
      "For k = 10, the average rmse = 10.452437841139657\n",
      "For k = 20, the average rmse = 10.642962232244766\n",
      "For k = 30, the average rmse = 10.809716983765423\n",
      "For k = 50, the average rmse = 10.992329252704964\n",
      "For k = 80, the average rmse = 11.064206293507606\n",
      "The rmse is lowest for k = 2 at = 10.177194623331173\n"
     ]
    }
   ],
   "source": [
    "best_params_svd = hyper_parameter_tuning_svd(dataset = filtered_jester_df, iterations = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rated items are: [15, 17, 27, 57, 67, 87]\n",
      "29    1.630473\n",
      "47    1.303044\n",
      "32    1.288816\n",
      "Name: 50, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "recommend_for_user = 50\n",
    "\n",
    "user_pred_df = pd.DataFrame(predictions)\n",
    "user_sel_pred_df = user_pred_df.loc[recommend_for_user].sort_values(ascending=False)\n",
    "\n",
    "user_df = pd.DataFrame(final_csr_matrix.toarray())\n",
    "selected = pd.DataFrame(user_df.loc[recommend_for_user])\n",
    "rated_movies = selected.loc[~(selected==0).all(axis=1)].index.values.tolist()\n",
    "\n",
    "recommended_movies = user_sel_pred_df.loc[~user_sel_pred_df.index.isin(rated_movies)]\n",
    "\n",
    "print('Rated items are:',rated_movies)\n",
    "print(recommended_movies[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Comparison KNN & SVD for Jester"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3.1 Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3.2 Recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer the subquestion: \"How do the KNN and SVD models compare?\" we will compare compare the rmse of both models for the same dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which type of RecSys based on CF could Netflix user to provide the most accurate recommendations to users?\n",
    "\n",
    "- How do KNN and SVD compare?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
